# Physics-Regularized-Neural-Surrogate-for-Printed-Memristors

This repository contains the full framework, dataset, and scripts to reproduce the experiments from:

**"Physics-Regularized Neural Surrogate Framework for Printed Memristors"**  

ðŸ“„ **Status:** *Accepted for publication in IEEE Access. Will post link once paper is published.*
![Figure1](https://github.com/user-attachments/assets/1bb95950-7589-4a6b-b2c3-7b978d529735)

---

## ðŸŒŸ Highlights

We propose the **physics-regularized neural surrogate (PRNS) framework** tailored for printed memristors, incorporating:

- âœ… **4.31Ã— accuracy improvement** over VTEAM baseline (RRMSE: 0.063 vs 0.273)
- âœ… **29% energy reduction** compared to phenomenological models
- âœ… **Device-to-device variability modeling** with parameter perturbation
- âœ… **Noise robustness**: stable under up to 10% noise (as tested)
- âœ… **Multi-mechanism conduction**: Ohmic, SCLC, and interfacial transport physics
- âœ… **Temperature-dependent reliability**: Arrhenius lifetime projections (E_a = 0.379 eV)
- âœ… **15 comprehensive validation experiments** spanning dynamics, reliability, and lifetime
- âœ… **SPICE-compatible export** via lookup tables for circuit integration

---

## ðŸ“‚ Repository Structure

```
printed-memristor-prns/
â”‚
â”œâ”€â”€ .devcontainer/
â”‚   â”œâ”€â”€ devcontainer.json                        # GitHub Codespaces configuration
â”‚   â””â”€â”€ post-create.sh                           # Environment setup script
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ printed_memristor_training_data.csv      # Pre-generated synthetic dataset
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_synthetic_data.py               # Dataset generator
â”‚   â”œâ”€â”€ mainPINNmodel.py                         # PRNS architecture + physics-regularized loss
â”‚   â”œâ”€â”€ TrainingFrameworkwithNoiseInjection.py   # Training utilities (variability + noise)
â”‚   â”œâ”€â”€ ExperimentalValidationFramework.py       # Evaluation metrics
â”‚   â”œâ”€â”€ VTEAMModelComparison.py                  # VTEAM baseline implementation
â”‚   â”œâ”€â”€ ResultsVisualization.py                  # Plotting utilities
â”‚   â”œâ”€â”€ CompleteExperimentalReproduction.py      # Orchestrates full experiments
â”‚   â”œâ”€â”€ ExtendedValidation.py                    # Extended validation experiments
â”‚   â”œâ”€â”€ balanced_simulation.py                   # Circuit simulation
â”‚   â”œâ”€â”€ export_pinn_to_spice.py                  # LUT export
â”‚   â”œâ”€â”€ run_pinn.py                              # Main entry point
â”‚   â”œâ”€â”€ exp1_dynamic_pulse_response.py       # Dynamic operation
â”‚   â”œâ”€â”€ exp2_write_read_cycles.py            # Non-destructive reads
â”‚   â”œâ”€â”€ exp3_energy_efficiency.py            # Energy comparison
â”‚   â”œâ”€â”€ exp4_multicell_variability.py        # Device-to-device variability
â”‚   â”œâ”€â”€ exp5_noise_robustness.py             # Noise tolerance
â”‚   â”œâ”€â”€ exp6_temperature_switching.py        # Temperature-dependent I-V
â”‚   â”œâ”€â”€ exp7_multilevel_retention.py         # Multi-level stability
â”‚   â”œâ”€â”€ exp8_temperature_retention.py        # Thermal drift analysis
â”‚   â”œâ”€â”€ exp9_endurance_cycling.py            # Cycling degradation
â”‚   â”œâ”€â”€ exp10_combined_reliability.py        # Cycle-temperature mapping
â”‚   â”œâ”€â”€ exp10_arrhenius_fit.py               # Helper: Arrhenius fitting
â”‚   â”œâ”€â”€ exp11_arrhenius_lifetime.py          # Lifetime projections
â”‚   â”œâ”€â”€ exp12_self_heating.py                # Electro-thermal coupling
â”‚   â”œâ”€â”€ exp13_bias_lifetime.py               # Bias-accelerated aging
â”‚   â””â”€â”€ exp14_15_reliability_and_acceleration.py  # Bottleneck analysis
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ main manuscript folders with experiments # Core validation results
â”‚   â””â”€â”€ supplementary_experiments/               # Results and Data generated from additional 15 experiments
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

```

---

## âš™ï¸ Installation

### **Option 1: GitHub Codespaces (Recommended for Quick Start)**

Launch a fully configured development environment in your browser with one click:

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/jurjsorinliviu/Physics-Regularized-Neural-Surrogate-for-Printed-Memristors)

**What's included:**

- ðŸ Python 3.10 with all dependencies pre-installed
- ðŸ“¦ TensorFlow, NumPy, Pandas, Matplotlib, SciPy
- ðŸ› ï¸ VS Code with Python, Jupyter, and linting extensions
- ðŸ“ Pre-configured results directories

**Getting started in Codespaces:**

1. Click the badge above or go to the repository and click **Code** â†’ **Codespaces** â†’ **Create codespace on main**

2. Wait for the container to build (~2-3 minutes on first launch)

3. The post-create script will automatically install all dependencies

4. Start running experiments immediately:

   ```bash
   python src/run_pinn.py --mode full --full-epochs 100 --results-dir results_test
   ```

### **Option 2: Local Installation**

**Clone and Install**

```bash
git clone https://github.com/jurjsorinliviu/Physics-Regularized-Neural-Surrogate-for-Printed-Memristors.git
cd Physics-Regularized-Neural-Surrogate-for-Printed-Memristors
pip install -r requirements.txt
```

**Requirements:**

```
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
scipy>=1.7.0
tensorflow>=2.9.0
```

---

## ðŸš€ Quick Start

### **1. Train PRNS Model (Best Configuration)**

```bash
python src/run_pinn.py --mode full \
  --full-epochs 800 \
  --full-hidden-layers 4 \
  --full-neurons 128 \
  --full-learning-rate 2e-4 \
  --full-noise-std 0.002 \
  --full-variability 0.05 \
  --full-max-physics-weight 0.1 \
  --full-trainable-params ohmic_conductance \
  --full-disable-concentration \
  --full-seed 42 \
  --results-dir results_best
```

**Expected Output**:

- Training converges in ~800 epochs
- Final RRMSE: 0.061 (vs VTEAM: 0.251)
- Results saved to results_best/

### **2. Cross-Validation (3 Seeds)**

```bash
python src/run_pinn.py --mode full \
  --full-epochs 800 \
  --full-hidden-layers 4 \
  --full-neurons 128 \
  --full-learning-rate 2e-4 \
  --full-noise-std 0.002 \
  --full-variability 0.05 \
  --full-max-physics-weight 0.1 \
  --full-trainable-params ohmic_conductance \
  --full-disable-concentration \
  --full-repeats 3 \
  --full-seed 40 \
  --results-dir results_cv \
  --no-plots
```

**Expected Output**:

- RRMSE: 0.115 Â± 0.062 across 3 seeds
- Statistical validation of robustness
- Update (Reviewer response): We ran an 8â€‘seed crossâ€‘validation (seeds 40â€“47). The median PRNS RRMSE is 0.065 with IQR 0.046â€“0.106, while VTEAM remains at 0.273 for all seeds. Even the worst PRNS seed outperforms VTEAM. These results confirm robustness beyond the original 3â€‘seed sweep.


### **Ablation Study (No Physics Constraints)**

```bash
python src/run_pinn.py --mode full \
  --full-epochs 800 \
  --full-hidden-layers 4 \
  --full-neurons 128 \
  --full-learning-rate 2e-4 \
  --full-noise-std 0.002 \
  --full-variability 0.05 \
  --full-trainable-params ohmic_conductance \
  --full-disable-concentration \
  --full-seed 42 \
  --results-dir results_ablation
```

**Expected Output**:

- ~1.6Ã— accuracy degradation without physics loss
- Validates importance of physics-informed constraints

---

## ðŸ”¬ Supplementary Experiments (n=15)

Comprehensive validation across dynamic operation, reliability, and lifetime projection.

### **Group 1: Dynamic Operation & Energy (Exp. 1-3)**

```bash
# Experiment 1: Dynamic pulse response (66.7 Hz, 50 pulses)
python src/exp1_dynamic_pulse_response.py
# Result: 34.5% current increase, analog potentiation validated

# Experiment 2: Write-read cycles (non-destructive reads)
python src/exp2_write_read_cycles.py
# Result: <3% CoV, stable read operations

# Experiment 3: Energy efficiency comparison
python src/exp3_energy_efficiency.py
# Result: 29% lower write energy than VTEAM (94.2 pJ vs 133.6 pJ)
```

### **Group 2: Variability & Robustness (Exp. 4-5)**

```bash
# Experiment 4: Multi-cell variability (5-device array)
python src/exp4_multicell_variability.py
# Result: 10Ã— amplification of parameter uncertainty (40.83% CoV)

# Experiment 5: Noise robustness (1-10% corruption)
python src/exp5_noise_robustness.py
# Result: 1.9-4.3Ã— lower error than baselines at 10% noise
```

### **Group 3: Temperature Physics (Exp. 6, 8)**

```bash
# Experiment 6: Temperature-dependent I-V (250-350 K)
python src/exp6_temperature_switching.py
# Result: 23.5% current modulation, E_a â‰ˆ 0.09 eV

# Experiment 8: Temperature-dependent retention
python src/exp8_temperature_retention.py
# Result: Arrhenius ordering validated, <1% drift at 350 K over measured window
```

### **Group 4: Retention & Endurance (Exp. 7, 9)**

```bash
# Experiment 7: Multi-level retention (10â¶ s, 3 levels)
python src/exp7_multilevel_retention.py
# Result: <3% drift over 11.6 days, Ï„ = 10â¶ s

# Experiment 9: Endurance cycling (200 SET/RESET)
python src/exp9_endurance_cycling.py
# Result: 29% window reduction, ~660-cycle lifetime
```

### **Group 5: Coupled Reliability & Lifetime (Exp. 10-15)**

```bash
# Experiment 10: Combined reliability mapping (cycle Ã— temperature)
python src/exp10_combined_reliability.py
# Result: Orthogonal failure modes identified

# Experiment 11: Arrhenius lifetime projection
python src/exp11_arrhenius_lifetime.py
# Result: E_a = 0.379 Â± 0.010 eV, RÂ² = 0.997

# Experiment 12: Self-heating dynamics (0.05-0.20 V)
python src/exp12_self_heating.py
# Result: Î”T = 5â€“10 K (realistic), ~4Ã— retention acceleration

# Experiment 13: Bias-accelerated lifetime
python src/exp13_bias_lifetime.py
# Result: 14-28% lifetime reduction with bias

# Experiments 14-15: Reliability bottleneck & acceleration
python src/exp14_15_reliability_and_acceleration.py
# Result: Endurance dominates (~4.8 h vs ~40 h retention)
```

**Outputs**: All results saved to results/supplementary_experiments/ with figures and CSV files.

---

## ðŸ”Ž Extended Validation (Digitized Experimental Curves)

Evaluate generalization on three published device classes:

```bash
python src/ExtendedValidation.py \
  --seeds 40 41 42 \
  --output-dir results/extended_validation
```

**Tested Devices**:

- âœ… Inkjet-printed IGZO (Ag/IGZO/ITO)
- âœ… Aerosol-jet MoSâ‚‚ (Ag/MoSâ‚‚/Ag)
- âœ… Paper-based MoSâ‚‚/graphene

**Result**: PRNS achieves lowest error on MoSâ‚‚ and paper datasets, remains competitive on IGZO.

---

## ðŸ”§ Circuit Integration

### **Step 1: Export PINN to SPICE-Compatible LUT**

```bash
python src/export_pinn_to_spice.py
```

**Outputs**:

- pinn_memristor_lut.txt (500Ã—50 grid, 25,000 points)
- lut_visualization.png (3D surface + I-V slices)
- circuit_schematic.png (1T1R cell diagram)

### **Step 2: Run 1T1R Circuit Simulation**

```bash
python src/balanced_simulation.py
```

**Expected Console Output**:

```bash
PINN Model (Physics-Informed, Gradual Switching):
  Initial state:    0.100
  Final state:      0.914
  State change:     0.814
  Peak current:     1500.00 Î¼A
  Write energy:     94.22 pJ

VTEAM Model (Phenomenological, Threshold-Based):
  Initial state:    0.100
  Final state:      0.991
  State change:     0.891
  Peak current:     1500.00 Î¼A
  Write energy:     133.63 pJ

Comparison:
  Energy ratio (PINN/VTEAM):    0.71Ã—
  Energy savings:                29%
```

### **Step 3: Integration with SPICE**

The exported LUT can be used in circuit simulators:

**ngspice (PWL interpolation):**

```bash
* Load LUT
.control
load pinn_memristor_lut.txt
...
.endc
```

**Verilog-A (analog block):**

```bash
// Inside analog block
I(p,n) <+ interpolate(lut_data, V(p,n), state);
```

**Customization**
Modify Circuit Parameters by editing balanced_simulation.py:

```bash
Line ~30-35: Memristor parameters
R_on = 1e3      # ON resistance (Ohm)
R_off = 100e3   # OFF resistance (Ohm)

Line ~38-43: Dynamics
alpha_pinn = 1e7   # PINN speed (increase for faster switching)
k_vteam = 1e8      # VTEAM speed

Line ~50-52: Voltage pulse
V_pulse = 1.5      # Pulse amplitude (V)
t_width = 100e-9   # Pulse duration (s)

Change Simulation Resolution
Line ~21: Time step
dt = 0.1e-9  # Decrease for finer resolution (but slower simulation)
```

---

## ðŸ“Š Dataset

### **Pre-Generated Dataset**

Training data (printed_memristor_training_data.csv) includes:

- 20,000 voltage-current pairs
- 4 PMMA concentrations: 5%, 10%, 15%, 20%
- Bipolar sweeps: -2.0 V to +2.0 V

### **Regenerate Dataset**

```bash
python src/generate_synthetic_data.py \
  --output data/printed_memristor_training_data.csv \
  --samples 20000 \
  --noise-std 0.01 \
  --variability 0.05 \
  --concentrations 5,10,15,20
```

**Options:**

- samples: Number of samples (default: 20,000)
- noise-std: Gaussian noise level
- variability: Device-to-device variation factor
- concentrations: PMMA concentrations (comma-separated)

---

## ðŸ“œ License

This project is licensed under the MIT License.  
